{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Conv2D,Dense,Conv2DTranspose, Reshape,Lambda,LeakyReLU,Flatten,Dropout,Activation,UpSampling2D\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "from keras.activations import tanh\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(\n",
    "    self, input_dims,\n",
    "        discriminator_conv_filters,\n",
    "        discriminator_conv_kernel_size,\n",
    "        discriminator_conv_strides,\n",
    "        discriminator_activation,\n",
    "        discriminator_dropout,\n",
    "        discriminator_lr,\n",
    "        generator_initial_dense_layer_size,\n",
    "        generator_upsample,\n",
    "        generator_conv_filters,\n",
    "        generator_conv_kernel_size,\n",
    "        generator_conv_strides,\n",
    "        generator_activation,\n",
    "        generator_dropout,\n",
    "        generator_lr,\n",
    "        optimizier,\n",
    "        latent_dims\n",
    "        \n",
    "    ):\n",
    "        \n",
    "        self.input_dims=input_dims\n",
    "        self.discriminator_conv_filters=discriminator_conv_filters\n",
    "        self.discriminator_conv_kernel_size=discriminator_conv_kernel_size\n",
    "        self.discriminator_conv_strides=discriminator_conv_strides\n",
    "        self.discriminator_activation=discriminator_activation\n",
    "        self.discriminator_dropout=discriminator_dropout\n",
    "        self.discriminator_lr=discriminator_lr\n",
    "        \n",
    "        self.generator_initial_dense_layer_size=generator_initial_dense_layer_size\n",
    "        self.generator_unsample=generator_upsample\n",
    "        self.generator_conv_filters=generator_conv_filters\n",
    "        self.generator_conv_kernel_size=generator_conv_kernel_size\n",
    "        self.generator_conv_strides=generator_conv_strides\n",
    "        self.generator_activation=generator_activation\n",
    "        self.generator_dropout=generator_dropout\n",
    "        self.generator_lr=generator_lr\n",
    "        self.optimizier=optimizier\n",
    "        self.latent_dims=latent_dims\n",
    "        \n",
    "        self.no_layers_discriminator=len(discriminator_conv_filters)\n",
    "        self.no_layers_generator=len(generator_conv_filters)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.d_loss=[]\n",
    "        self.g_loss=[]\n",
    "        \n",
    "        self._build_discriminator()\n",
    "        self._build_generator()\n",
    "        \n",
    "        \n",
    "    def get_activation(self,activation):\n",
    "        if activation =='leaky_relu':\n",
    "            layer=LeakyRelu(alpha=0.2)\n",
    "        else:\n",
    "            layer=Activation(activation) \n",
    "        return layer\n",
    "    \n",
    "    def _build_discriminator(self):\n",
    "        discriminator_input=Input(shape=self.input_dims,name='discriminator_input')\n",
    "        \n",
    "        x=discriminator_input # I always forget this...everytime, and then have to deal with errors I've never even heard of...I dont know why this happens everytime.\n",
    "        \n",
    "        for i in range(self.no_layers_discriminator):\n",
    "            x=Conv2D(\n",
    "                filters=self.discriminator_conv_filters[i],\n",
    "                kernel_size=self.discriminator_conv_kernel_size[i],\n",
    "                strides=self.discriminator_conv_strides[i],\n",
    "                padding='same',\n",
    "                name='discriminator_layer_'+str(i),\n",
    "\n",
    "                \n",
    "            )(x)\n",
    "            \n",
    "            x=self.get_activation(self.discriminator_activation)(x)\n",
    "            if self.discriminator_dropout:\n",
    "                x=Dropout(rate=self.discriminator_dropout)(x)\n",
    "            x=Flatten(x)\n",
    "            \n",
    "            discriminator_output=Dense(1,activation='sigmoid')(x)\n",
    "            \n",
    "            self.discriminator=Model(discriminator_input,discriminator_output)\n",
    "            \n",
    "            \n",
    "            \n",
    "        def _build_generator(self): #tricky part\n",
    "            # mind the layers\n",
    "            generator_input=Input(shape=(self.latent_dims,),name='generator_input')\n",
    "            #input done\n",
    "            # latent dims -> dense (shape maybe 7*7*64 ?) or np.prod(dense) ? -> reshape? ->connect to generator then...\n",
    "            #input (latent dim size) to dense of number given in input ???\n",
    "            # give the size of dense of generator as ip\n",
    "            # make dense of np.prod(ip)\n",
    "            # connect generator_input to this np.prod(dense) layer\n",
    "            # now try -\n",
    "            #  -- connect this to a smaller dense ?\n",
    "            #  -- connect this to reshape?\n",
    "            #  -- connect directly to unsample ?\n",
    "             \n",
    "                \n",
    "            x= generator_input # dont forget this as I do always\n",
    "            \n",
    "            # trying small dens first, checking working\n",
    "            #x=Dense(512)(x)\n",
    "            \n",
    "            # trying reshape to ip size specified\n",
    "            x=Dense(np.prod(self.generator_initial_dense_layer_size))(x)\n",
    "            x=self.get_activation(self.generator_activation)(x)\n",
    "            \n",
    "            x=Reshape(generator_initial_dense_layer_size)(x)\n",
    "            \n",
    "            if self.generator_dropout:\n",
    "                x=Dropout(self.generator_dropout)(x)\n",
    "            # reshape to prod of dims\n",
    "            # connect to dense ? or connect to unsample ?\n",
    "            \n",
    "            \n",
    "            for  i in range(self.no_layers_generator):\n",
    "                if self.generator_upsample == 2:\n",
    "                    x=UpSampling2D()(x)\n",
    "                    x=Conv2D(\n",
    "                        filters=self.generator_conv_filters[i],\n",
    "                        kernel_size=self.generator_conv_kernel_size[i],\n",
    "                        strides=self.generator_conv_strides[i],\n",
    "                        padding='same',\n",
    "                        name='generator_layer_'+str(i)\n",
    "                    \n",
    "                    )(x)\n",
    "                else:\n",
    "                    Conv2DTranspose(\n",
    "                        filters=self.generator_conv_filters[i],\n",
    "                        kernel_size=self.generator_conv_kernel_size[i],\n",
    "                        strides=self.generator_conv_strides[i],\n",
    "                        padding='same',\n",
    "                        name='generator_layer_'+str(i)                        \n",
    "                    \n",
    "                    )(x)\n",
    "                    \n",
    "            if i<self.no_layers_generator-1:\n",
    "                x=self.get_activation(self.generator_activation)(x)\n",
    "            \n",
    "            else:\n",
    "                x=Activation('tanh')(x)\n",
    "                \n",
    "            generator_output=x\n",
    "            \n",
    "            \n",
    "            self.generator=Model(generator_input,generator_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        def get_opti(self, lr):\n",
    "            if self.optimizer=='adam':\n",
    "                opti=Adam(lr=lr,beta=0.5)\n",
    "            elif self.optiimzer=='rmsprop':\n",
    "                opti=RMSprop(lr=lr)\n",
    "            else:\n",
    "                opti=Adam(lr=lr)\n",
    "            return opti\n",
    "        \n",
    "        \n",
    "        def set_trainable(self,model_name,bool_value):\n",
    "            '''\n",
    "            for freezing dicrimimnaor training when required\n",
    "            '''\n",
    "            model_name.trainable=bool_value\n",
    "            for l in model_name.layers:\n",
    "                l.trainable=bool_value\n",
    "                \n",
    "        def _build_adversial(self):\n",
    "            # compile discriminator\n",
    "            self.discriminator.compile(\n",
    "            optimizer=self.get_opti(self.discriminator_lr),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            self.set_trainable(self.discriminator,False)\n",
    "            \n",
    "            model_input=Input(shape=(self.latent_dims,),name='model_input')\n",
    "            model_output=self.discriminator(self.generator(model_input))\n",
    "            self.model=Model(model_input,model_output)\n",
    "            \n",
    "            \n",
    "            self.model.compile(optimizer=self.get_opti(self.discriminator_lr),\n",
    "                              loss='binary_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "            \n",
    "            self.set_trainable(self.discriminator,True)\n",
    "            \n",
    "            \n",
    "        def train_discrimminator(self,x_train,batch_size,using_generator):\n",
    "            valid=np.ones((batch_size,1))\n",
    "            fake=np.zeros((batch_size,1))\n",
    "            \n",
    "            #train on real images\n",
    "            if using_generator:\n",
    "                true_imgs=next(x_train)[0]\n",
    "                if true_imgs.shape[0]!=batch_size:\n",
    "                    true_imgs=next(x_train)[0]\n",
    "            else:\n",
    "                idx=np.random.randn(0,x_train.shape[0],batch_size)\n",
    "                true_imgs=x_train[idx]\n",
    "                \n",
    "            #train on generated images\n",
    "            noise=np.random.normal(0,1,(batch_size,self.latent_dims))\n",
    "            \n",
    "            gen_images=self.generator.predict(noise)\n",
    "            \n",
    "            d_loss_real,d_acc_real=self.discriminator.train_on_batch(true_imgs,valid)\n",
    "            d_loss_fake,d_acc_fake=self.discriminator.train_on_batch(gen_imgs,fake)\n",
    "            \n",
    "            d_loss=0.5*(d_loss_real+d_loss_fake) #avg of real n fake\n",
    "            d_acc=0.5*(d_acc_real+d_acc_fake)\n",
    "            return [d_loss,d_loss_real,d_loss_fake,d_acc,d_acc_real,d_acc_fake]\n",
    "        \n",
    "        \n",
    "        def train_generator(self,batch_size):\n",
    "            valid=np.ones((batch_size,1))\n",
    "            noise=np.random.normal(0,1,(batch_size,latent_dims))\n",
    "            return self.model.train_on_batch(noise,valid)\n",
    "        \n",
    "        \n",
    "        def train(self,x_train,batch_size,epochs,run_folder,print_every_n_batch=50,using_generator=False):\n",
    "            for epoch in range(self.epoch,slf.epoch+epochs):\n",
    "                d=self.train_discriminator(x_train,batch_size,using_generator)\n",
    "                g=self.train_generator(batch_size)\n",
    "                \n",
    "                print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n",
    "                self.d_losses.append(d)\n",
    "                self.g_losses.append(g)\n",
    "                \n",
    "                \n",
    "                if epoch % print_every_n_batch==0:\n",
    "                    self.sample_images(run_folder)\n",
    "                    self.model.save_weights(os.path.join(run_folder,'weights/weights-%d.h5'%(epoch)))\n",
    "                    self.model.save_weights(os.path.join(run_folder,'weights/weights.h5'))\n",
    "                    self.save_model(run_folder)\n",
    "                self.epoch+=1\n",
    "            \n",
    "            \n",
    "        def save(self,folder):\n",
    "            with open(os.path.join(folder,'params.pkl'),'wb')as f:\n",
    "                pkl.dump([\n",
    "                    self.latent_dims,\n",
    "                    self.discriminator_conv_filters,\n",
    "                    self.discriminator_conv_kernel_size,\n",
    "                    self.discriminator_conv_strides,\n",
    "                    self.discriminator_activation,\n",
    "                    self.discriminator_dorpout,\n",
    "                    self.discriminator_initial_dense_layer_size,\n",
    "                    self.discriminator_lr,\n",
    "                    self.discriminator_input,\n",
    "                    self.discriminator_output,\n",
    "                ])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganskernel",
   "language": "python",
   "name": "ganskernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
