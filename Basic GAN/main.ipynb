{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext importnb\n",
    "import GANstructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../data/camel/full_numpy_bitmap_camel.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FOLDER='run/'\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER,'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER,'weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(path):\n",
    "    dataset_size=5000\n",
    "    dataset=np.load(path)\n",
    "    no_of_images=dataset.shape[0]\n",
    "    dataset=dataset.reshape(no_of_images,28,28,1)\n",
    "    dataset=dataset[:dataset_size]\n",
    "    dataset = dataset.astype('float32') / 255.0\n",
    "    '''\n",
    "    test 1\n",
    "    arr = np.arange(10)\n",
    "    np.random.shuffle(arr)\n",
    "    print(arr)\n",
    "    '''\n",
    "    np.random.shuffle(dataset)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=loader(path)\n",
    "x=np.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GANstructure.GAN(input_dims = (28,28,1)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout = 0.4\n",
    "        , discriminator_lr = 0.0008\n",
    "        , generator_initial_dense_layer_size = (7, 7, 64)\n",
    "        , generator_upsample = [2,2,1,1]\n",
    "        , generator_conv_filters = [128,64, 64,1]\n",
    "        , generator_conv_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_strides = [1,1, 1, 1]\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout = None\n",
    "        , generator_lr = 0.0004\n",
    "        , optimizer = 'rmsprop'\n",
    "        , latent_dims = 100\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_layer_0 (Conv2 (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_layer_1 (Conv2 (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_layer_2 (Conv2 (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_layer_3 (Conv2 (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_layer_0 (Conv2D)   (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_layer_1 (Conv2D)   (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_layer_2 (Conv2DTra (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_layer_3 (Conv2DTra (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 830,593\n",
      "Trainable params: 830,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=dataset[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Data\\appData2\\anaconda3\\envs\\gans\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.967)(R 0.698, F 1.236)] [D acc: (0.164)(0.328, 0.000)] [G loss: 0.688] [G acc: 1.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Data\\appData2\\anaconda3\\envs\\gans\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: (0.708)(R 0.680, F 0.736)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.680] [G acc: 1.000]\n",
      "2 [D loss: (0.697)(R 0.684, F 0.710)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.685] [G acc: 1.000]\n",
      "3 [D loss: (0.694)(R 0.683, F 0.705)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.690] [G acc: 0.844]\n",
      "4 [D loss: (0.692)(R 0.683, F 0.702)] [D acc: (0.508)(1.000, 0.016)] [G loss: 0.696] [G acc: 0.234]\n",
      "5 [D loss: (0.689)(R 0.680, F 0.698)] [D acc: (0.594)(1.000, 0.188)] [G loss: 0.712] [G acc: 0.000]\n",
      "6 [D loss: (0.688)(R 0.670, F 0.705)] [D acc: (0.578)(1.000, 0.156)] [G loss: 0.759] [G acc: 0.000]\n",
      "7 [D loss: (0.649)(R 0.666, F 0.631)] [D acc: (0.977)(0.953, 1.000)] [G loss: 1.096] [G acc: 0.000]\n",
      "8 [D loss: (1.433)(R 0.516, F 2.350)] [D acc: (0.500)(1.000, 0.000)] [G loss: 1.382] [G acc: 0.000]\n",
      "9 [D loss: (0.463)(R 0.621, F 0.306)] [D acc: (0.938)(0.875, 1.000)] [G loss: 2.533] [G acc: 0.000]\n",
      "10 [D loss: (0.301)(R 0.512, F 0.090)] [D acc: (0.930)(0.859, 1.000)] [G loss: 4.323] [G acc: 0.000]\n",
      "11 [D loss: (0.160)(R 0.293, F 0.027)] [D acc: (0.977)(0.953, 1.000)] [G loss: 5.569] [G acc: 0.000]\n",
      "12 [D loss: (0.066)(R 0.119, F 0.013)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.694] [G acc: 0.000]\n",
      "13 [D loss: (0.016)(R 0.028, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.235] [G acc: 0.000]\n",
      "14 [D loss: (0.005)(R 0.009, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.205] [G acc: 0.000]\n",
      "15 [D loss: (1.087)(R 0.005, F 2.168)] [D acc: (0.508)(1.000, 0.016)] [G loss: 4.375] [G acc: 0.000]\n",
      "16 [D loss: (1.247)(R 1.569, F 0.924)] [D acc: (0.062)(0.125, 0.000)] [G loss: 0.671] [G acc: 0.828]\n",
      "17 [D loss: (0.597)(R 0.433, F 0.761)] [D acc: (0.469)(0.891, 0.047)] [G loss: 0.720] [G acc: 0.203]\n",
      "18 [D loss: (0.479)(R 0.326, F 0.632)] [D acc: (0.945)(0.953, 0.938)] [G loss: 0.847] [G acc: 0.000]\n",
      "19 [D loss: (0.459)(R 0.198, F 0.720)] [D acc: (0.672)(0.984, 0.359)] [G loss: 0.879] [G acc: 0.000]\n",
      "20 [D loss: (0.460)(R 0.307, F 0.614)] [D acc: (0.922)(0.891, 0.953)] [G loss: 1.041] [G acc: 0.000]\n",
      "21 [D loss: (0.449)(R 0.147, F 0.751)] [D acc: (0.555)(0.969, 0.141)] [G loss: 0.996] [G acc: 0.000]\n",
      "22 [D loss: (0.293)(R 0.167, F 0.419)] [D acc: (0.961)(0.922, 1.000)] [G loss: 1.583] [G acc: 0.000]\n",
      "23 [D loss: (0.285)(R 0.065, F 0.505)] [D acc: (0.977)(0.969, 0.984)] [G loss: 1.783] [G acc: 0.000]\n",
      "24 [D loss: (0.372)(R 0.083, F 0.661)] [D acc: (0.844)(0.984, 0.703)] [G loss: 1.218] [G acc: 0.000]\n",
      "25 [D loss: (0.186)(R 0.074, F 0.299)] [D acc: (0.992)(0.984, 1.000)] [G loss: 2.154] [G acc: 0.000]\n",
      "26 [D loss: (0.183)(R 0.063, F 0.304)] [D acc: (0.984)(0.969, 1.000)] [G loss: 3.309] [G acc: 0.000]\n",
      "27 [D loss: (0.271)(R 0.174, F 0.369)] [D acc: (0.969)(0.938, 1.000)] [G loss: 2.115] [G acc: 0.000]\n",
      "28 [D loss: (0.091)(R 0.034, F 0.148)] [D acc: (0.992)(0.984, 1.000)] [G loss: 3.022] [G acc: 0.000]\n",
      "29 [D loss: (0.063)(R 0.021, F 0.105)] [D acc: (0.992)(0.984, 1.000)] [G loss: 4.354] [G acc: 0.000]\n",
      "30 [D loss: (0.025)(R 0.009, F 0.040)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.556] [G acc: 0.000]\n",
      "31 [D loss: (0.013)(R 0.007, F 0.019)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.241] [G acc: 0.000]\n",
      "32 [D loss: (0.600)(R 0.050, F 1.151)] [D acc: (0.703)(0.969, 0.438)] [G loss: 11.093] [G acc: 0.000]\n",
      "33 [D loss: (4.519)(R 8.590, F 0.449)] [D acc: (0.469)(0.000, 0.938)] [G loss: 1.945] [G acc: 0.000]\n",
      "34 [D loss: (0.139)(R 0.041, F 0.236)] [D acc: (0.984)(1.000, 0.969)] [G loss: 1.932] [G acc: 0.000]\n",
      "35 [D loss: (0.338)(R 0.024, F 0.652)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.616] [G acc: 0.016]\n",
      "36 [D loss: (0.290)(R 0.277, F 0.303)] [D acc: (0.922)(0.906, 0.938)] [G loss: 2.195] [G acc: 0.016]\n",
      "37 [D loss: (0.813)(R 0.088, F 1.539)] [D acc: (0.633)(0.969, 0.297)] [G loss: 1.573] [G acc: 0.016]\n",
      "38 [D loss: (0.898)(R 0.304, F 1.492)] [D acc: (0.562)(0.906, 0.219)] [G loss: 1.569] [G acc: 0.016]\n",
      "39 [D loss: (0.645)(R 0.582, F 0.709)] [D acc: (0.672)(0.750, 0.594)] [G loss: 2.932] [G acc: 0.000]\n",
      "40 [D loss: (0.590)(R 0.383, F 0.798)] [D acc: (0.680)(0.891, 0.469)] [G loss: 1.614] [G acc: 0.016]\n",
      "41 [D loss: (0.606)(R 0.455, F 0.758)] [D acc: (0.641)(0.828, 0.453)] [G loss: 1.921] [G acc: 0.000]\n",
      "42 [D loss: (0.604)(R 0.443, F 0.766)] [D acc: (0.680)(0.859, 0.500)] [G loss: 3.638] [G acc: 0.000]\n",
      "43 [D loss: (0.468)(R 0.570, F 0.365)] [D acc: (0.812)(0.656, 0.969)] [G loss: 2.788] [G acc: 0.000]\n",
      "44 [D loss: (0.203)(R 0.243, F 0.163)] [D acc: (0.945)(0.922, 0.969)] [G loss: 3.253] [G acc: 0.000]\n",
      "45 [D loss: (0.230)(R 0.168, F 0.292)] [D acc: (0.969)(0.953, 0.984)] [G loss: 5.664] [G acc: 0.000]\n",
      "46 [D loss: (0.430)(R 0.500, F 0.361)] [D acc: (0.797)(0.734, 0.859)] [G loss: 3.228] [G acc: 0.000]\n",
      "47 [D loss: (0.148)(R 0.060, F 0.236)] [D acc: (0.984)(1.000, 0.969)] [G loss: 4.295] [G acc: 0.000]\n",
      "48 [D loss: (0.108)(R 0.116, F 0.100)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.218] [G acc: 0.000]\n",
      "49 [D loss: (0.736)(R 0.111, F 1.361)] [D acc: (0.766)(0.969, 0.562)] [G loss: 6.518] [G acc: 0.000]\n",
      "50 [D loss: (2.017)(R 3.116, F 0.918)] [D acc: (0.188)(0.000, 0.375)] [G loss: 0.880] [G acc: 0.328]\n",
      "51 [D loss: (0.508)(R 0.117, F 0.898)] [D acc: (0.648)(1.000, 0.297)] [G loss: 0.899] [G acc: 0.203]\n",
      "52 [D loss: (0.530)(R 0.201, F 0.858)] [D acc: (0.680)(1.000, 0.359)] [G loss: 0.947] [G acc: 0.156]\n",
      "53 [D loss: (0.525)(R 0.280, F 0.771)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.128] [G acc: 0.109]\n",
      "54 [D loss: (0.465)(R 0.274, F 0.656)] [D acc: (0.828)(0.984, 0.672)] [G loss: 2.909] [G acc: 0.000]\n",
      "55 [D loss: (0.222)(R 0.338, F 0.105)] [D acc: (0.977)(0.953, 1.000)] [G loss: 3.346] [G acc: 0.000]\n",
      "56 [D loss: (0.251)(R 0.145, F 0.358)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.633] [G acc: 0.016]\n",
      "57 [D loss: (0.245)(R 0.161, F 0.328)] [D acc: (0.930)(0.969, 0.891)] [G loss: 3.402] [G acc: 0.000]\n",
      "58 [D loss: (0.376)(R 0.279, F 0.472)] [D acc: (0.852)(0.875, 0.828)] [G loss: 4.000] [G acc: 0.000]\n",
      "59 [D loss: (0.398)(R 0.371, F 0.426)] [D acc: (0.828)(0.844, 0.812)] [G loss: 4.276] [G acc: 0.000]\n",
      "60 [D loss: (0.201)(R 0.217, F 0.186)] [D acc: (0.953)(0.938, 0.969)] [G loss: 4.024] [G acc: 0.000]\n",
      "61 [D loss: (0.424)(R 0.114, F 0.735)] [D acc: (0.781)(0.969, 0.594)] [G loss: 9.617] [G acc: 0.000]\n",
      "62 [D loss: (2.216)(R 4.267, F 0.166)] [D acc: (0.516)(0.031, 1.000)] [G loss: 3.485] [G acc: 0.000]\n",
      "63 [D loss: (0.152)(R 0.019, F 0.285)] [D acc: (0.984)(1.000, 0.969)] [G loss: 2.370] [G acc: 0.016]\n",
      "64 [D loss: (0.183)(R 0.020, F 0.346)] [D acc: (0.961)(1.000, 0.922)] [G loss: 2.065] [G acc: 0.000]\n",
      "65 [D loss: (0.257)(R 0.065, F 0.450)] [D acc: (0.883)(0.969, 0.797)] [G loss: 2.715] [G acc: 0.000]\n",
      "66 [D loss: (0.120)(R 0.101, F 0.138)] [D acc: (0.984)(0.969, 1.000)] [G loss: 2.962] [G acc: 0.000]\n",
      "67 [D loss: (0.107)(R 0.065, F 0.149)] [D acc: (0.984)(0.969, 1.000)] [G loss: 4.285] [G acc: 0.000]\n",
      "68 [D loss: (0.102)(R 0.145, F 0.060)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.833] [G acc: 0.000]\n",
      "69 [D loss: (0.070)(R 0.016, F 0.124)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.917] [G acc: 0.000]\n",
      "70 [D loss: (0.213)(R 0.198, F 0.228)] [D acc: (0.922)(0.922, 0.922)] [G loss: 5.050] [G acc: 0.000]\n",
      "71 [D loss: (0.104)(R 0.136, F 0.073)] [D acc: (0.969)(0.953, 0.984)] [G loss: 4.795] [G acc: 0.000]\n",
      "72 [D loss: (0.022)(R 0.010, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.965] [G acc: 0.000]\n",
      "73 [D loss: (0.246)(R 0.026, F 0.466)] [D acc: (0.883)(0.984, 0.781)] [G loss: 15.158] [G acc: 0.000]\n",
      "74 [D loss: (3.313)(R 2.996, F 3.631)] [D acc: (0.023)(0.047, 0.000)] [G loss: 3.751] [G acc: 0.000]\n",
      "75 [D loss: (0.164)(R 0.222, F 0.106)] [D acc: (0.969)(0.938, 1.000)] [G loss: 2.674] [G acc: 0.000]\n",
      "76 [D loss: (0.199)(R 0.127, F 0.271)] [D acc: (0.977)(0.969, 0.984)] [G loss: 2.677] [G acc: 0.000]\n",
      "77 [D loss: (0.223)(R 0.131, F 0.316)] [D acc: (0.961)(0.969, 0.953)] [G loss: 3.222] [G acc: 0.000]\n",
      "78 [D loss: (0.128)(R 0.144, F 0.112)] [D acc: (0.977)(0.969, 0.984)] [G loss: 3.360] [G acc: 0.000]\n",
      "79 [D loss: (0.184)(R 0.159, F 0.210)] [D acc: (0.977)(0.953, 1.000)] [G loss: 3.666] [G acc: 0.000]\n",
      "80 [D loss: (0.183)(R 0.182, F 0.184)] [D acc: (0.977)(0.953, 1.000)] [G loss: 4.344] [G acc: 0.000]\n",
      "81 [D loss: (0.295)(R 0.184, F 0.405)] [D acc: (0.883)(0.922, 0.844)] [G loss: 5.885] [G acc: 0.000]\n",
      "82 [D loss: (0.383)(R 0.746, F 0.020)] [D acc: (0.805)(0.609, 1.000)] [G loss: 4.187] [G acc: 0.000]\n",
      "83 [D loss: (0.019)(R 0.019, F 0.020)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.472] [G acc: 0.000]\n",
      "84 [D loss: (0.276)(R 0.012, F 0.541)] [D acc: (0.859)(1.000, 0.719)] [G loss: 6.197] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 [D loss: (0.095)(R 0.164, F 0.025)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.111] [G acc: 0.000]\n",
      "86 [D loss: (0.060)(R 0.070, F 0.049)] [D acc: (0.992)(0.984, 1.000)] [G loss: 3.817] [G acc: 0.016]\n",
      "87 [D loss: (0.957)(R 0.110, F 1.803)] [D acc: (0.711)(0.953, 0.469)] [G loss: 8.012] [G acc: 0.000]\n",
      "88 [D loss: (1.794)(R 2.582, F 1.006)] [D acc: (0.227)(0.016, 0.438)] [G loss: 1.675] [G acc: 0.062]\n",
      "89 [D loss: (0.728)(R 0.290, F 1.167)] [D acc: (0.609)(0.969, 0.250)] [G loss: 1.371] [G acc: 0.016]\n",
      "90 [D loss: (0.467)(R 0.393, F 0.540)] [D acc: (0.836)(0.938, 0.734)] [G loss: 1.315] [G acc: 0.078]\n",
      "91 [D loss: (0.440)(R 0.332, F 0.547)] [D acc: (0.844)(0.922, 0.766)] [G loss: 3.748] [G acc: 0.000]\n",
      "92 [D loss: (0.551)(R 0.459, F 0.642)] [D acc: (0.719)(0.828, 0.609)] [G loss: 1.557] [G acc: 0.000]\n",
      "93 [D loss: (0.409)(R 0.344, F 0.473)] [D acc: (0.859)(0.906, 0.812)] [G loss: 2.420] [G acc: 0.000]\n",
      "94 [D loss: (0.365)(R 0.277, F 0.453)] [D acc: (0.883)(0.953, 0.812)] [G loss: 6.467] [G acc: 0.000]\n",
      "95 [D loss: (0.405)(R 0.718, F 0.093)] [D acc: (0.812)(0.625, 1.000)] [G loss: 3.756] [G acc: 0.000]\n",
      "96 [D loss: (0.129)(R 0.127, F 0.131)] [D acc: (0.984)(0.969, 1.000)] [G loss: 3.123] [G acc: 0.000]\n",
      "97 [D loss: (0.051)(R 0.068, F 0.034)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.181] [G acc: 0.000]\n",
      "98 [D loss: (0.513)(R 0.050, F 0.976)] [D acc: (0.688)(1.000, 0.375)] [G loss: 5.569] [G acc: 0.000]\n",
      "99 [D loss: (1.110)(R 1.396, F 0.824)] [D acc: (0.344)(0.250, 0.438)] [G loss: 1.551] [G acc: 0.000]\n",
      "100 [D loss: (0.355)(R 0.241, F 0.470)] [D acc: (0.945)(1.000, 0.891)] [G loss: 2.074] [G acc: 0.000]\n",
      "101 [D loss: (0.308)(R 0.251, F 0.365)] [D acc: (0.945)(0.938, 0.953)] [G loss: 2.698] [G acc: 0.000]\n",
      "102 [D loss: (0.352)(R 0.372, F 0.332)] [D acc: (0.891)(0.875, 0.906)] [G loss: 4.362] [G acc: 0.000]\n",
      "103 [D loss: (0.356)(R 0.461, F 0.252)] [D acc: (0.867)(0.781, 0.953)] [G loss: 3.917] [G acc: 0.000]\n",
      "104 [D loss: (0.122)(R 0.214, F 0.030)] [D acc: (0.961)(0.922, 1.000)] [G loss: 4.097] [G acc: 0.000]\n",
      "105 [D loss: (0.450)(R 0.024, F 0.876)] [D acc: (0.734)(1.000, 0.469)] [G loss: 5.595] [G acc: 0.000]\n",
      "106 [D loss: (1.158)(R 2.086, F 0.230)] [D acc: (0.531)(0.062, 1.000)] [G loss: 2.646] [G acc: 0.000]\n",
      "107 [D loss: (0.172)(R 0.163, F 0.181)] [D acc: (0.992)(0.984, 1.000)] [G loss: 2.406] [G acc: 0.000]\n",
      "108 [D loss: (0.351)(R 0.043, F 0.659)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.403] [G acc: 0.000]\n",
      "109 [D loss: (0.534)(R 0.968, F 0.099)] [D acc: (0.742)(0.484, 1.000)] [G loss: 3.153] [G acc: 0.000]\n",
      "110 [D loss: (0.309)(R 0.057, F 0.560)] [D acc: (0.852)(1.000, 0.703)] [G loss: 1.976] [G acc: 0.016]\n",
      "111 [D loss: (0.377)(R 0.169, F 0.586)] [D acc: (0.828)(0.984, 0.672)] [G loss: 2.806] [G acc: 0.000]\n",
      "112 [D loss: (0.508)(R 0.474, F 0.543)] [D acc: (0.750)(0.781, 0.719)] [G loss: 2.534] [G acc: 0.000]\n",
      "113 [D loss: (0.534)(R 0.491, F 0.576)] [D acc: (0.758)(0.828, 0.688)] [G loss: 2.512] [G acc: 0.000]\n",
      "114 [D loss: (0.435)(R 0.440, F 0.429)] [D acc: (0.828)(0.828, 0.828)] [G loss: 2.851] [G acc: 0.000]\n",
      "115 [D loss: (0.237)(R 0.315, F 0.159)] [D acc: (0.953)(0.922, 0.984)] [G loss: 3.923] [G acc: 0.000]\n",
      "116 [D loss: (0.138)(R 0.095, F 0.182)] [D acc: (0.984)(1.000, 0.969)] [G loss: 4.025] [G acc: 0.000]\n",
      "117 [D loss: (0.256)(R 0.185, F 0.327)] [D acc: (0.922)(0.984, 0.859)] [G loss: 5.838] [G acc: 0.000]\n",
      "118 [D loss: (0.710)(R 0.859, F 0.562)] [D acc: (0.641)(0.562, 0.719)] [G loss: 4.531] [G acc: 0.000]\n",
      "119 [D loss: (0.139)(R 0.230, F 0.049)] [D acc: (0.984)(0.969, 1.000)] [G loss: 3.847] [G acc: 0.000]\n",
      "120 [D loss: (0.028)(R 0.033, F 0.023)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.170] [G acc: 0.000]\n",
      "121 [D loss: (0.040)(R 0.042, F 0.038)] [D acc: (0.992)(0.984, 1.000)] [G loss: 3.924] [G acc: 0.000]\n",
      "122 [D loss: (0.040)(R 0.011, F 0.069)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.245] [G acc: 0.000]\n",
      "123 [D loss: (0.481)(R 0.016, F 0.946)] [D acc: (0.742)(1.000, 0.484)] [G loss: 7.112] [G acc: 0.000]\n",
      "124 [D loss: (2.026)(R 3.926, F 0.125)] [D acc: (0.500)(0.000, 1.000)] [G loss: 2.995] [G acc: 0.000]\n",
      "125 [D loss: (0.209)(R 0.143, F 0.275)] [D acc: (0.969)(1.000, 0.938)] [G loss: 2.199] [G acc: 0.000]\n",
      "126 [D loss: (0.251)(R 0.085, F 0.418)] [D acc: (0.922)(1.000, 0.844)] [G loss: 2.939] [G acc: 0.000]\n",
      "127 [D loss: (0.158)(R 0.281, F 0.036)] [D acc: (0.945)(0.891, 1.000)] [G loss: 3.880] [G acc: 0.000]\n",
      "128 [D loss: (0.169)(R 0.037, F 0.301)] [D acc: (0.945)(1.000, 0.891)] [G loss: 3.500] [G acc: 0.000]\n",
      "129 [D loss: (0.211)(R 0.161, F 0.262)] [D acc: (0.953)(0.969, 0.938)] [G loss: 3.677] [G acc: 0.000]\n",
      "130 [D loss: (0.164)(R 0.154, F 0.173)] [D acc: (0.977)(0.969, 0.984)] [G loss: 5.511] [G acc: 0.000]\n",
      "131 [D loss: (0.176)(R 0.274, F 0.077)] [D acc: (0.938)(0.891, 0.984)] [G loss: 3.565] [G acc: 0.000]\n",
      "132 [D loss: (0.458)(R 0.013, F 0.903)] [D acc: (0.797)(1.000, 0.594)] [G loss: 4.492] [G acc: 0.000]\n",
      "133 [D loss: (0.856)(R 1.360, F 0.352)] [D acc: (0.555)(0.281, 0.828)] [G loss: 3.359] [G acc: 0.000]\n",
      "134 [D loss: (0.179)(R 0.113, F 0.246)] [D acc: (0.977)(0.984, 0.969)] [G loss: 2.207] [G acc: 0.000]\n",
      "135 [D loss: (0.400)(R 0.115, F 0.685)] [D acc: (0.766)(0.984, 0.547)] [G loss: 3.961] [G acc: 0.000]\n",
      "136 [D loss: (0.610)(R 1.123, F 0.096)] [D acc: (0.641)(0.281, 1.000)] [G loss: 3.453] [G acc: 0.000]\n",
      "137 [D loss: (0.242)(R 0.103, F 0.382)] [D acc: (0.930)(0.984, 0.875)] [G loss: 2.475] [G acc: 0.016]\n",
      "138 [D loss: (0.665)(R 0.235, F 1.094)] [D acc: (0.641)(0.953, 0.328)] [G loss: 2.766] [G acc: 0.000]\n",
      "139 [D loss: (0.707)(R 0.883, F 0.531)] [D acc: (0.594)(0.438, 0.750)] [G loss: 1.757] [G acc: 0.016]\n",
      "140 [D loss: (0.499)(R 0.395, F 0.602)] [D acc: (0.773)(0.875, 0.672)] [G loss: 2.178] [G acc: 0.000]\n",
      "141 [D loss: (0.414)(R 0.446, F 0.381)] [D acc: (0.812)(0.781, 0.844)] [G loss: 3.439] [G acc: 0.000]\n",
      "142 [D loss: (0.363)(R 0.480, F 0.246)] [D acc: (0.867)(0.781, 0.953)] [G loss: 3.636] [G acc: 0.000]\n",
      "143 [D loss: (0.353)(R 0.282, F 0.423)] [D acc: (0.867)(0.875, 0.859)] [G loss: 3.182] [G acc: 0.000]\n",
      "144 [D loss: (0.310)(R 0.220, F 0.400)] [D acc: (0.898)(0.938, 0.859)] [G loss: 3.136] [G acc: 0.000]\n",
      "145 [D loss: (0.514)(R 0.403, F 0.625)] [D acc: (0.766)(0.781, 0.750)] [G loss: 3.933] [G acc: 0.000]\n",
      "146 [D loss: (0.458)(R 0.735, F 0.181)] [D acc: (0.812)(0.641, 0.984)] [G loss: 3.362] [G acc: 0.000]\n",
      "147 [D loss: (0.399)(R 0.137, F 0.661)] [D acc: (0.789)(0.953, 0.625)] [G loss: 2.675] [G acc: 0.000]\n",
      "148 [D loss: (0.636)(R 0.633, F 0.639)] [D acc: (0.672)(0.656, 0.688)] [G loss: 3.347] [G acc: 0.000]\n",
      "149 [D loss: (0.451)(R 0.545, F 0.357)] [D acc: (0.797)(0.719, 0.875)] [G loss: 5.398] [G acc: 0.000]\n",
      "150 [D loss: (0.346)(R 0.431, F 0.262)] [D acc: (0.891)(0.812, 0.969)] [G loss: 3.254] [G acc: 0.000]\n",
      "151 [D loss: (0.237)(R 0.160, F 0.314)] [D acc: (0.938)(0.969, 0.906)] [G loss: 2.647] [G acc: 0.000]\n",
      "152 [D loss: (0.497)(R 0.448, F 0.547)] [D acc: (0.773)(0.797, 0.750)] [G loss: 4.253] [G acc: 0.000]\n",
      "153 [D loss: (0.401)(R 0.544, F 0.259)] [D acc: (0.844)(0.734, 0.953)] [G loss: 3.572] [G acc: 0.000]\n",
      "154 [D loss: (0.363)(R 0.261, F 0.465)] [D acc: (0.883)(0.938, 0.828)] [G loss: 2.793] [G acc: 0.000]\n",
      "155 [D loss: (0.381)(R 0.289, F 0.472)] [D acc: (0.820)(0.875, 0.766)] [G loss: 2.806] [G acc: 0.000]\n",
      "156 [D loss: (0.484)(R 0.469, F 0.500)] [D acc: (0.773)(0.797, 0.750)] [G loss: 3.194] [G acc: 0.000]\n",
      "157 [D loss: (0.382)(R 0.612, F 0.152)] [D acc: (0.828)(0.672, 0.984)] [G loss: 5.376] [G acc: 0.000]\n",
      "158 [D loss: (0.372)(R 0.287, F 0.456)] [D acc: (0.828)(0.859, 0.797)] [G loss: 3.724] [G acc: 0.000]\n",
      "159 [D loss: (0.375)(R 0.248, F 0.501)] [D acc: (0.859)(0.922, 0.797)] [G loss: 2.808] [G acc: 0.016]\n",
      "160 [D loss: (0.362)(R 0.407, F 0.317)] [D acc: (0.875)(0.828, 0.922)] [G loss: 3.260] [G acc: 0.000]\n",
      "161 [D loss: (0.515)(R 0.287, F 0.743)] [D acc: (0.734)(0.859, 0.609)] [G loss: 3.820] [G acc: 0.000]\n",
      "162 [D loss: (0.482)(R 0.704, F 0.260)] [D acc: (0.758)(0.578, 0.938)] [G loss: 3.307] [G acc: 0.000]\n",
      "163 [D loss: (0.373)(R 0.370, F 0.376)] [D acc: (0.844)(0.828, 0.859)] [G loss: 2.628] [G acc: 0.000]\n",
      "164 [D loss: (0.389)(R 0.196, F 0.582)] [D acc: (0.836)(0.938, 0.734)] [G loss: 2.821] [G acc: 0.000]\n",
      "165 [D loss: (0.496)(R 0.561, F 0.432)] [D acc: (0.797)(0.734, 0.859)] [G loss: 2.376] [G acc: 0.000]\n",
      "166 [D loss: (0.442)(R 0.440, F 0.443)] [D acc: (0.789)(0.781, 0.797)] [G loss: 2.492] [G acc: 0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 [D loss: (0.387)(R 0.395, F 0.379)] [D acc: (0.852)(0.844, 0.859)] [G loss: 3.981] [G acc: 0.000]\n",
      "168 [D loss: (0.370)(R 0.560, F 0.181)] [D acc: (0.828)(0.688, 0.969)] [G loss: 5.764] [G acc: 0.000]\n",
      "169 [D loss: (0.246)(R 0.253, F 0.240)] [D acc: (0.930)(0.906, 0.953)] [G loss: 2.619] [G acc: 0.000]\n",
      "170 [D loss: (0.202)(R 0.123, F 0.281)] [D acc: (0.945)(0.953, 0.938)] [G loss: 3.188] [G acc: 0.000]\n",
      "171 [D loss: (0.276)(R 0.231, F 0.322)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.514] [G acc: 0.000]\n",
      "172 [D loss: (0.554)(R 0.485, F 0.622)] [D acc: (0.656)(0.656, 0.656)] [G loss: 3.448] [G acc: 0.000]\n",
      "173 [D loss: (0.545)(R 0.637, F 0.453)] [D acc: (0.734)(0.656, 0.812)] [G loss: 3.622] [G acc: 0.000]\n",
      "174 [D loss: (0.442)(R 0.552, F 0.332)] [D acc: (0.789)(0.672, 0.906)] [G loss: 3.707] [G acc: 0.000]\n",
      "175 [D loss: (0.399)(R 0.337, F 0.461)] [D acc: (0.797)(0.812, 0.781)] [G loss: 2.887] [G acc: 0.000]\n",
      "176 [D loss: (0.351)(R 0.318, F 0.384)] [D acc: (0.867)(0.906, 0.828)] [G loss: 2.992] [G acc: 0.000]\n",
      "177 [D loss: (0.352)(R 0.363, F 0.342)] [D acc: (0.852)(0.812, 0.891)] [G loss: 3.024] [G acc: 0.000]\n",
      "178 [D loss: (0.477)(R 0.465, F 0.488)] [D acc: (0.742)(0.719, 0.766)] [G loss: 3.017] [G acc: 0.000]\n",
      "179 [D loss: (0.379)(R 0.504, F 0.255)] [D acc: (0.844)(0.734, 0.953)] [G loss: 3.961] [G acc: 0.000]\n",
      "180 [D loss: (0.326)(R 0.498, F 0.155)] [D acc: (0.852)(0.766, 0.938)] [G loss: 7.440] [G acc: 0.000]\n",
      "181 [D loss: (0.172)(R 0.151, F 0.193)] [D acc: (0.961)(0.969, 0.953)] [G loss: 2.679] [G acc: 0.000]\n",
      "182 [D loss: (0.216)(R 0.157, F 0.274)] [D acc: (0.938)(0.969, 0.906)] [G loss: 3.732] [G acc: 0.000]\n",
      "183 [D loss: (0.108)(R 0.172, F 0.044)] [D acc: (0.961)(0.922, 1.000)] [G loss: 4.633] [G acc: 0.000]\n",
      "184 [D loss: (0.083)(R 0.127, F 0.039)] [D acc: (0.977)(0.953, 1.000)] [G loss: 4.975] [G acc: 0.000]\n",
      "185 [D loss: (0.229)(R 0.074, F 0.383)] [D acc: (0.945)(0.984, 0.906)] [G loss: 5.577] [G acc: 0.000]\n",
      "186 [D loss: (0.607)(R 0.473, F 0.741)] [D acc: (0.703)(0.734, 0.672)] [G loss: 4.758] [G acc: 0.000]\n",
      "187 [D loss: (0.515)(R 0.789, F 0.241)] [D acc: (0.750)(0.578, 0.922)] [G loss: 2.957] [G acc: 0.016]\n",
      "188 [D loss: (0.262)(R 0.128, F 0.396)] [D acc: (0.906)(0.984, 0.828)] [G loss: 2.913] [G acc: 0.000]\n",
      "189 [D loss: (0.245)(R 0.275, F 0.216)] [D acc: (0.914)(0.891, 0.938)] [G loss: 2.978] [G acc: 0.000]\n",
      "190 [D loss: (0.178)(R 0.180, F 0.177)] [D acc: (0.945)(0.922, 0.969)] [G loss: 3.594] [G acc: 0.000]\n",
      "191 [D loss: (0.187)(R 0.190, F 0.183)] [D acc: (0.953)(0.938, 0.969)] [G loss: 3.467] [G acc: 0.000]\n",
      "192 [D loss: (0.183)(R 0.169, F 0.198)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.324] [G acc: 0.000]\n",
      "193 [D loss: (0.383)(R 0.387, F 0.379)] [D acc: (0.828)(0.844, 0.812)] [G loss: 3.912] [G acc: 0.031]\n",
      "194 [D loss: (0.292)(R 0.326, F 0.257)] [D acc: (0.891)(0.859, 0.922)] [G loss: 5.443] [G acc: 0.000]\n",
      "195 [D loss: (0.461)(R 0.361, F 0.561)] [D acc: (0.773)(0.828, 0.719)] [G loss: 4.162] [G acc: 0.000]\n",
      "196 [D loss: (0.316)(R 0.410, F 0.222)] [D acc: (0.875)(0.812, 0.938)] [G loss: 3.534] [G acc: 0.000]\n",
      "197 [D loss: (0.230)(R 0.178, F 0.283)] [D acc: (0.922)(0.938, 0.906)] [G loss: 4.392] [G acc: 0.000]\n",
      "198 [D loss: (0.209)(R 0.266, F 0.151)] [D acc: (0.930)(0.906, 0.953)] [G loss: 3.545] [G acc: 0.000]\n",
      "199 [D loss: (0.446)(R 0.264, F 0.628)] [D acc: (0.789)(0.891, 0.688)] [G loss: 4.421] [G acc: 0.000]\n",
      "200 [D loss: (0.770)(R 1.037, F 0.503)] [D acc: (0.648)(0.516, 0.781)] [G loss: 3.158] [G acc: 0.016]\n",
      "201 [D loss: (0.285)(R 0.345, F 0.224)] [D acc: (0.914)(0.859, 0.969)] [G loss: 3.252] [G acc: 0.000]\n",
      "202 [D loss: (0.225)(R 0.105, F 0.344)] [D acc: (0.875)(0.969, 0.781)] [G loss: 5.450] [G acc: 0.000]\n",
      "203 [D loss: (0.360)(R 0.523, F 0.196)] [D acc: (0.844)(0.719, 0.969)] [G loss: 3.728] [G acc: 0.016]\n",
      "204 [D loss: (0.209)(R 0.105, F 0.312)] [D acc: (0.922)(0.969, 0.875)] [G loss: 3.559] [G acc: 0.000]\n",
      "205 [D loss: (0.414)(R 0.288, F 0.541)] [D acc: (0.820)(0.906, 0.734)] [G loss: 3.960] [G acc: 0.000]\n",
      "206 [D loss: (0.453)(R 0.543, F 0.364)] [D acc: (0.789)(0.703, 0.875)] [G loss: 3.082] [G acc: 0.031]\n",
      "207 [D loss: (0.284)(R 0.276, F 0.292)] [D acc: (0.875)(0.891, 0.859)] [G loss: 3.528] [G acc: 0.016]\n",
      "208 [D loss: (0.374)(R 0.452, F 0.297)] [D acc: (0.828)(0.797, 0.859)] [G loss: 3.583] [G acc: 0.000]\n",
      "209 [D loss: (0.266)(R 0.295, F 0.237)] [D acc: (0.891)(0.859, 0.922)] [G loss: 3.856] [G acc: 0.000]\n",
      "210 [D loss: (0.372)(R 0.335, F 0.408)] [D acc: (0.805)(0.812, 0.797)] [G loss: 4.128] [G acc: 0.000]\n",
      "211 [D loss: (0.394)(R 0.426, F 0.362)] [D acc: (0.844)(0.828, 0.859)] [G loss: 3.862] [G acc: 0.000]\n",
      "212 [D loss: (0.445)(R 0.461, F 0.428)] [D acc: (0.820)(0.828, 0.812)] [G loss: 3.373] [G acc: 0.000]\n",
      "213 [D loss: (0.392)(R 0.368, F 0.416)] [D acc: (0.859)(0.859, 0.859)] [G loss: 3.080] [G acc: 0.016]\n",
      "214 [D loss: (0.309)(R 0.391, F 0.226)] [D acc: (0.867)(0.828, 0.906)] [G loss: 6.276] [G acc: 0.000]\n",
      "215 [D loss: (0.222)(R 0.303, F 0.141)] [D acc: (0.930)(0.891, 0.969)] [G loss: 3.866] [G acc: 0.000]\n",
      "216 [D loss: (0.233)(R 0.151, F 0.316)] [D acc: (0.898)(0.953, 0.844)] [G loss: 5.339] [G acc: 0.000]\n",
      "217 [D loss: (0.322)(R 0.299, F 0.345)] [D acc: (0.875)(0.844, 0.906)] [G loss: 4.865] [G acc: 0.000]\n",
      "218 [D loss: (0.402)(R 0.338, F 0.467)] [D acc: (0.828)(0.859, 0.797)] [G loss: 4.348] [G acc: 0.000]\n",
      "219 [D loss: (0.291)(R 0.313, F 0.269)] [D acc: (0.867)(0.828, 0.906)] [G loss: 4.313] [G acc: 0.000]\n",
      "220 [D loss: (0.286)(R 0.260, F 0.312)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.847] [G acc: 0.000]\n",
      "221 [D loss: (0.403)(R 0.340, F 0.466)] [D acc: (0.797)(0.828, 0.766)] [G loss: 4.223] [G acc: 0.000]\n",
      "222 [D loss: (0.474)(R 0.675, F 0.273)] [D acc: (0.820)(0.750, 0.891)] [G loss: 3.423] [G acc: 0.000]\n",
      "223 [D loss: (0.318)(R 0.289, F 0.348)] [D acc: (0.859)(0.844, 0.875)] [G loss: 3.591] [G acc: 0.000]\n",
      "224 [D loss: (0.359)(R 0.326, F 0.393)] [D acc: (0.836)(0.859, 0.812)] [G loss: 3.431] [G acc: 0.016]\n",
      "225 [D loss: (0.465)(R 0.472, F 0.457)] [D acc: (0.805)(0.797, 0.812)] [G loss: 3.562] [G acc: 0.000]\n",
      "226 [D loss: (0.393)(R 0.495, F 0.291)] [D acc: (0.797)(0.703, 0.891)] [G loss: 3.867] [G acc: 0.016]\n",
      "227 [D loss: (0.344)(R 0.204, F 0.485)] [D acc: (0.859)(0.953, 0.766)] [G loss: 3.819] [G acc: 0.000]\n",
      "228 [D loss: (0.357)(R 0.443, F 0.271)] [D acc: (0.844)(0.797, 0.891)] [G loss: 3.412] [G acc: 0.016]\n",
      "229 [D loss: (0.282)(R 0.164, F 0.401)] [D acc: (0.891)(0.969, 0.812)] [G loss: 3.907] [G acc: 0.000]\n",
      "230 [D loss: (0.285)(R 0.375, F 0.196)] [D acc: (0.883)(0.812, 0.953)] [G loss: 3.813] [G acc: 0.000]\n",
      "231 [D loss: (0.325)(R 0.278, F 0.371)] [D acc: (0.844)(0.859, 0.828)] [G loss: 4.333] [G acc: 0.000]\n",
      "232 [D loss: (0.342)(R 0.357, F 0.327)] [D acc: (0.812)(0.812, 0.812)] [G loss: 4.388] [G acc: 0.000]\n",
      "233 [D loss: (0.394)(R 0.402, F 0.386)] [D acc: (0.820)(0.812, 0.828)] [G loss: 4.013] [G acc: 0.000]\n",
      "234 [D loss: (0.282)(R 0.351, F 0.213)] [D acc: (0.883)(0.859, 0.906)] [G loss: 4.532] [G acc: 0.000]\n",
      "235 [D loss: (0.354)(R 0.324, F 0.384)] [D acc: (0.820)(0.828, 0.812)] [G loss: 3.471] [G acc: 0.000]\n",
      "236 [D loss: (0.251)(R 0.284, F 0.217)] [D acc: (0.898)(0.875, 0.922)] [G loss: 3.704] [G acc: 0.000]\n",
      "237 [D loss: (0.288)(R 0.275, F 0.301)] [D acc: (0.914)(0.875, 0.953)] [G loss: 4.028] [G acc: 0.016]\n",
      "238 [D loss: (0.251)(R 0.193, F 0.308)] [D acc: (0.891)(0.906, 0.875)] [G loss: 4.361] [G acc: 0.000]\n",
      "239 [D loss: (0.491)(R 0.441, F 0.541)] [D acc: (0.789)(0.797, 0.781)] [G loss: 3.586] [G acc: 0.000]\n",
      "240 [D loss: (0.400)(R 0.459, F 0.341)] [D acc: (0.828)(0.766, 0.891)] [G loss: 3.540] [G acc: 0.000]\n",
      "241 [D loss: (0.286)(R 0.203, F 0.369)] [D acc: (0.930)(0.953, 0.906)] [G loss: 4.570] [G acc: 0.000]\n",
      "242 [D loss: (0.295)(R 0.304, F 0.286)] [D acc: (0.891)(0.891, 0.891)] [G loss: 4.520] [G acc: 0.000]\n",
      "243 [D loss: (0.178)(R 0.170, F 0.186)] [D acc: (0.930)(0.938, 0.922)] [G loss: 4.219] [G acc: 0.000]\n",
      "244 [D loss: (0.191)(R 0.192, F 0.190)] [D acc: (0.945)(0.953, 0.938)] [G loss: 4.599] [G acc: 0.000]\n",
      "245 [D loss: (0.315)(R 0.304, F 0.326)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.131] [G acc: 0.000]\n",
      "246 [D loss: (0.350)(R 0.434, F 0.266)] [D acc: (0.828)(0.750, 0.906)] [G loss: 4.944] [G acc: 0.000]\n",
      "247 [D loss: (0.470)(R 0.317, F 0.623)] [D acc: (0.758)(0.859, 0.656)] [G loss: 4.343] [G acc: 0.000]\n",
      "248 [D loss: (0.479)(R 0.560, F 0.399)] [D acc: (0.820)(0.734, 0.906)] [G loss: 4.018] [G acc: 0.000]\n",
      "249 [D loss: (0.326)(R 0.348, F 0.304)] [D acc: (0.844)(0.828, 0.859)] [G loss: 3.134] [G acc: 0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 [D loss: (0.290)(R 0.212, F 0.369)] [D acc: (0.891)(0.938, 0.844)] [G loss: 3.482] [G acc: 0.000]\n",
      "251 [D loss: (0.275)(R 0.317, F 0.233)] [D acc: (0.898)(0.844, 0.953)] [G loss: 3.649] [G acc: 0.000]\n",
      "252 [D loss: (0.311)(R 0.257, F 0.365)] [D acc: (0.891)(0.922, 0.859)] [G loss: 4.312] [G acc: 0.000]\n",
      "253 [D loss: (0.237)(R 0.246, F 0.228)] [D acc: (0.891)(0.875, 0.906)] [G loss: 4.139] [G acc: 0.000]\n",
      "254 [D loss: (0.295)(R 0.284, F 0.306)] [D acc: (0.883)(0.906, 0.859)] [G loss: 4.384] [G acc: 0.000]\n",
      "255 [D loss: (0.348)(R 0.493, F 0.204)] [D acc: (0.852)(0.781, 0.922)] [G loss: 5.136] [G acc: 0.000]\n",
      "256 [D loss: (0.318)(R 0.179, F 0.458)] [D acc: (0.836)(0.938, 0.734)] [G loss: 5.646] [G acc: 0.000]\n",
      "257 [D loss: (0.390)(R 0.562, F 0.217)] [D acc: (0.836)(0.750, 0.922)] [G loss: 4.095] [G acc: 0.000]\n",
      "258 [D loss: (0.252)(R 0.122, F 0.382)] [D acc: (0.898)(0.969, 0.828)] [G loss: 4.219] [G acc: 0.000]\n",
      "259 [D loss: (0.294)(R 0.407, F 0.180)] [D acc: (0.875)(0.812, 0.938)] [G loss: 4.838] [G acc: 0.000]\n",
      "260 [D loss: (0.168)(R 0.149, F 0.188)] [D acc: (0.930)(0.938, 0.922)] [G loss: 4.744] [G acc: 0.000]\n",
      "261 [D loss: (0.218)(R 0.184, F 0.251)] [D acc: (0.930)(0.938, 0.922)] [G loss: 4.890] [G acc: 0.000]\n",
      "262 [D loss: (0.208)(R 0.226, F 0.190)] [D acc: (0.930)(0.906, 0.953)] [G loss: 4.747] [G acc: 0.000]\n",
      "263 [D loss: (0.269)(R 0.177, F 0.361)] [D acc: (0.914)(0.969, 0.859)] [G loss: 4.550] [G acc: 0.000]\n",
      "264 [D loss: (0.350)(R 0.485, F 0.216)] [D acc: (0.859)(0.812, 0.906)] [G loss: 4.380] [G acc: 0.000]\n",
      "265 [D loss: (0.364)(R 0.316, F 0.413)] [D acc: (0.820)(0.859, 0.781)] [G loss: 5.565] [G acc: 0.000]\n",
      "266 [D loss: (0.428)(R 0.520, F 0.337)] [D acc: (0.789)(0.719, 0.859)] [G loss: 4.212] [G acc: 0.000]\n",
      "267 [D loss: (0.196)(R 0.226, F 0.166)] [D acc: (0.922)(0.891, 0.953)] [G loss: 3.815] [G acc: 0.000]\n",
      "268 [D loss: (0.138)(R 0.092, F 0.184)] [D acc: (0.953)(0.984, 0.922)] [G loss: 4.276] [G acc: 0.000]\n",
      "269 [D loss: (0.225)(R 0.246, F 0.203)] [D acc: (0.891)(0.859, 0.922)] [G loss: 3.967] [G acc: 0.016]\n",
      "270 [D loss: (0.168)(R 0.146, F 0.191)] [D acc: (0.945)(0.938, 0.953)] [G loss: 4.381] [G acc: 0.000]\n",
      "271 [D loss: (0.715)(R 0.588, F 0.842)] [D acc: (0.727)(0.734, 0.719)] [G loss: 5.691] [G acc: 0.000]\n",
      "272 [D loss: (0.217)(R 0.318, F 0.117)] [D acc: (0.914)(0.875, 0.953)] [G loss: 3.905] [G acc: 0.016]\n",
      "273 [D loss: (0.401)(R 0.187, F 0.615)] [D acc: (0.844)(0.922, 0.766)] [G loss: 4.968] [G acc: 0.000]\n",
      "274 [D loss: (0.436)(R 0.552, F 0.320)] [D acc: (0.820)(0.766, 0.875)] [G loss: 3.815] [G acc: 0.000]\n",
      "275 [D loss: (0.207)(R 0.262, F 0.153)] [D acc: (0.922)(0.891, 0.953)] [G loss: 3.460] [G acc: 0.031]\n",
      "276 [D loss: (0.369)(R 0.253, F 0.485)] [D acc: (0.852)(0.859, 0.844)] [G loss: 4.324] [G acc: 0.016]\n",
      "277 [D loss: (0.278)(R 0.371, F 0.184)] [D acc: (0.906)(0.844, 0.969)] [G loss: 3.731] [G acc: 0.000]\n",
      "278 [D loss: (0.215)(R 0.153, F 0.277)] [D acc: (0.945)(0.953, 0.938)] [G loss: 3.792] [G acc: 0.000]\n",
      "279 [D loss: (0.311)(R 0.321, F 0.301)] [D acc: (0.898)(0.891, 0.906)] [G loss: 4.122] [G acc: 0.000]\n",
      "280 [D loss: (0.253)(R 0.280, F 0.226)] [D acc: (0.898)(0.875, 0.922)] [G loss: 5.488] [G acc: 0.000]\n",
      "281 [D loss: (0.160)(R 0.215, F 0.104)] [D acc: (0.961)(0.922, 1.000)] [G loss: 4.496] [G acc: 0.000]\n",
      "282 [D loss: (0.202)(R 0.159, F 0.245)] [D acc: (0.922)(0.938, 0.906)] [G loss: 4.131] [G acc: 0.016]\n",
      "283 [D loss: (0.204)(R 0.178, F 0.231)] [D acc: (0.922)(0.953, 0.891)] [G loss: 4.621] [G acc: 0.000]\n",
      "284 [D loss: (0.353)(R 0.249, F 0.456)] [D acc: (0.789)(0.844, 0.734)] [G loss: 5.296] [G acc: 0.000]\n",
      "285 [D loss: (0.597)(R 0.722, F 0.472)] [D acc: (0.750)(0.688, 0.812)] [G loss: 4.868] [G acc: 0.000]\n",
      "286 [D loss: (0.302)(R 0.280, F 0.324)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.394] [G acc: 0.000]\n",
      "287 [D loss: (0.167)(R 0.150, F 0.184)] [D acc: (0.953)(0.969, 0.938)] [G loss: 4.164] [G acc: 0.000]\n",
      "288 [D loss: (0.411)(R 0.463, F 0.360)] [D acc: (0.844)(0.828, 0.859)] [G loss: 3.653] [G acc: 0.016]\n",
      "289 [D loss: (0.164)(R 0.129, F 0.200)] [D acc: (0.961)(0.984, 0.938)] [G loss: 4.268] [G acc: 0.000]\n",
      "290 [D loss: (0.201)(R 0.175, F 0.227)] [D acc: (0.914)(0.906, 0.922)] [G loss: 4.105] [G acc: 0.000]\n",
      "291 [D loss: (0.350)(R 0.381, F 0.319)] [D acc: (0.844)(0.828, 0.859)] [G loss: 4.312] [G acc: 0.000]\n",
      "292 [D loss: (0.303)(R 0.330, F 0.277)] [D acc: (0.859)(0.844, 0.875)] [G loss: 4.678] [G acc: 0.000]\n",
      "293 [D loss: (0.213)(R 0.301, F 0.124)] [D acc: (0.930)(0.875, 0.984)] [G loss: 5.028] [G acc: 0.000]\n",
      "294 [D loss: (0.288)(R 0.168, F 0.409)] [D acc: (0.898)(0.938, 0.859)] [G loss: 4.538] [G acc: 0.000]\n",
      "295 [D loss: (0.612)(R 0.643, F 0.581)] [D acc: (0.734)(0.766, 0.703)] [G loss: 4.441] [G acc: 0.000]\n",
      "296 [D loss: (0.339)(R 0.508, F 0.170)] [D acc: (0.875)(0.797, 0.953)] [G loss: 3.507] [G acc: 0.000]\n",
      "297 [D loss: (0.164)(R 0.121, F 0.206)] [D acc: (0.961)(0.984, 0.938)] [G loss: 3.363] [G acc: 0.000]\n",
      "298 [D loss: (0.179)(R 0.155, F 0.204)] [D acc: (0.914)(0.938, 0.891)] [G loss: 4.183] [G acc: 0.000]\n",
      "299 [D loss: (0.300)(R 0.339, F 0.260)] [D acc: (0.898)(0.891, 0.906)] [G loss: 4.134] [G acc: 0.000]\n",
      "300 [D loss: (0.456)(R 0.393, F 0.519)] [D acc: (0.805)(0.812, 0.797)] [G loss: 3.664] [G acc: 0.000]\n",
      "301 [D loss: (0.310)(R 0.265, F 0.355)] [D acc: (0.867)(0.906, 0.828)] [G loss: 4.112] [G acc: 0.000]\n",
      "302 [D loss: (0.266)(R 0.277, F 0.254)] [D acc: (0.898)(0.891, 0.906)] [G loss: 3.987] [G acc: 0.000]\n",
      "303 [D loss: (0.295)(R 0.342, F 0.249)] [D acc: (0.867)(0.812, 0.922)] [G loss: 4.725] [G acc: 0.000]\n",
      "304 [D loss: (0.166)(R 0.185, F 0.146)] [D acc: (0.922)(0.906, 0.938)] [G loss: 4.661] [G acc: 0.000]\n",
      "305 [D loss: (0.172)(R 0.156, F 0.188)] [D acc: (0.930)(0.922, 0.938)] [G loss: 4.009] [G acc: 0.016]\n",
      "306 [D loss: (0.236)(R 0.159, F 0.314)] [D acc: (0.891)(0.938, 0.844)] [G loss: 4.984] [G acc: 0.000]\n",
      "307 [D loss: (0.485)(R 0.615, F 0.355)] [D acc: (0.797)(0.719, 0.875)] [G loss: 4.891] [G acc: 0.000]\n",
      "308 [D loss: (0.660)(R 0.506, F 0.813)] [D acc: (0.688)(0.766, 0.609)] [G loss: 4.959] [G acc: 0.000]\n",
      "309 [D loss: (0.447)(R 0.701, F 0.192)] [D acc: (0.805)(0.625, 0.984)] [G loss: 3.323] [G acc: 0.000]\n",
      "310 [D loss: (0.314)(R 0.231, F 0.397)] [D acc: (0.852)(0.938, 0.766)] [G loss: 3.425] [G acc: 0.000]\n",
      "311 [D loss: (0.214)(R 0.212, F 0.216)] [D acc: (0.922)(0.938, 0.906)] [G loss: 3.520] [G acc: 0.000]\n",
      "312 [D loss: (0.223)(R 0.174, F 0.272)] [D acc: (0.922)(0.938, 0.906)] [G loss: 3.677] [G acc: 0.000]\n",
      "313 [D loss: (0.330)(R 0.312, F 0.348)] [D acc: (0.867)(0.859, 0.875)] [G loss: 3.662] [G acc: 0.031]\n",
      "314 [D loss: (0.314)(R 0.353, F 0.275)] [D acc: (0.859)(0.859, 0.859)] [G loss: 4.930] [G acc: 0.000]\n",
      "315 [D loss: (0.133)(R 0.182, F 0.083)] [D acc: (0.977)(0.953, 1.000)] [G loss: 3.814] [G acc: 0.000]\n",
      "316 [D loss: (0.213)(R 0.105, F 0.322)] [D acc: (0.938)(0.984, 0.891)] [G loss: 4.512] [G acc: 0.000]\n",
      "317 [D loss: (0.367)(R 0.422, F 0.312)] [D acc: (0.828)(0.812, 0.844)] [G loss: 4.124] [G acc: 0.000]\n",
      "318 [D loss: (0.327)(R 0.261, F 0.394)] [D acc: (0.844)(0.891, 0.797)] [G loss: 5.155] [G acc: 0.000]\n",
      "319 [D loss: (0.537)(R 0.891, F 0.183)] [D acc: (0.805)(0.688, 0.922)] [G loss: 3.839] [G acc: 0.000]\n",
      "320 [D loss: (0.319)(R 0.241, F 0.397)] [D acc: (0.844)(0.906, 0.781)] [G loss: 3.780] [G acc: 0.016]\n",
      "321 [D loss: (0.278)(R 0.248, F 0.308)] [D acc: (0.875)(0.891, 0.859)] [G loss: 4.453] [G acc: 0.000]\n",
      "322 [D loss: (0.148)(R 0.204, F 0.092)] [D acc: (0.961)(0.938, 0.984)] [G loss: 3.720] [G acc: 0.000]\n",
      "323 [D loss: (0.216)(R 0.164, F 0.267)] [D acc: (0.922)(0.969, 0.875)] [G loss: 4.220] [G acc: 0.000]\n",
      "324 [D loss: (0.328)(R 0.279, F 0.377)] [D acc: (0.852)(0.875, 0.828)] [G loss: 4.753] [G acc: 0.016]\n",
      "325 [D loss: (0.578)(R 0.571, F 0.585)] [D acc: (0.734)(0.719, 0.750)] [G loss: 4.281] [G acc: 0.000]\n",
      "326 [D loss: (0.457)(R 0.521, F 0.393)] [D acc: (0.766)(0.734, 0.797)] [G loss: 3.947] [G acc: 0.000]\n",
      "327 [D loss: (0.213)(R 0.203, F 0.223)] [D acc: (0.914)(0.922, 0.906)] [G loss: 3.753] [G acc: 0.000]\n",
      "328 [D loss: (0.200)(R 0.219, F 0.180)] [D acc: (0.930)(0.906, 0.953)] [G loss: 3.973] [G acc: 0.000]\n",
      "329 [D loss: (0.329)(R 0.254, F 0.404)] [D acc: (0.875)(0.906, 0.844)] [G loss: 4.358] [G acc: 0.000]\n",
      "330 [D loss: (0.282)(R 0.361, F 0.203)] [D acc: (0.867)(0.812, 0.922)] [G loss: 4.043] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331 [D loss: (0.248)(R 0.186, F 0.310)] [D acc: (0.883)(0.906, 0.859)] [G loss: 4.371] [G acc: 0.016]\n",
      "332 [D loss: (0.404)(R 0.432, F 0.376)] [D acc: (0.828)(0.750, 0.906)] [G loss: 4.192] [G acc: 0.031]\n",
      "333 [D loss: (0.247)(R 0.161, F 0.333)] [D acc: (0.898)(0.969, 0.828)] [G loss: 4.457] [G acc: 0.000]\n",
      "334 [D loss: (0.270)(R 0.443, F 0.098)] [D acc: (0.922)(0.859, 0.984)] [G loss: 3.906] [G acc: 0.016]\n",
      "335 [D loss: (0.298)(R 0.175, F 0.421)] [D acc: (0.875)(0.906, 0.844)] [G loss: 4.626] [G acc: 0.000]\n",
      "336 [D loss: (0.406)(R 0.380, F 0.432)] [D acc: (0.820)(0.812, 0.828)] [G loss: 4.103] [G acc: 0.016]\n",
      "337 [D loss: (0.310)(R 0.398, F 0.222)] [D acc: (0.867)(0.812, 0.922)] [G loss: 3.583] [G acc: 0.031]\n",
      "338 [D loss: (0.361)(R 0.302, F 0.419)] [D acc: (0.820)(0.891, 0.750)] [G loss: 3.852] [G acc: 0.016]\n",
      "339 [D loss: (0.380)(R 0.469, F 0.290)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.924] [G acc: 0.000]\n",
      "340 [D loss: (0.254)(R 0.237, F 0.270)] [D acc: (0.930)(0.969, 0.891)] [G loss: 3.702] [G acc: 0.016]\n",
      "341 [D loss: (0.435)(R 0.408, F 0.463)] [D acc: (0.773)(0.781, 0.766)] [G loss: 3.821] [G acc: 0.000]\n",
      "342 [D loss: (0.336)(R 0.359, F 0.313)] [D acc: (0.891)(0.891, 0.891)] [G loss: 4.639] [G acc: 0.000]\n",
      "343 [D loss: (0.269)(R 0.384, F 0.154)] [D acc: (0.875)(0.781, 0.969)] [G loss: 4.168] [G acc: 0.000]\n",
      "344 [D loss: (0.267)(R 0.141, F 0.393)] [D acc: (0.867)(0.938, 0.797)] [G loss: 4.245] [G acc: 0.000]\n",
      "345 [D loss: (0.305)(R 0.331, F 0.280)] [D acc: (0.852)(0.875, 0.828)] [G loss: 4.088] [G acc: 0.016]\n",
      "346 [D loss: (0.211)(R 0.178, F 0.243)] [D acc: (0.922)(0.953, 0.891)] [G loss: 3.998] [G acc: 0.016]\n",
      "347 [D loss: (0.243)(R 0.220, F 0.266)] [D acc: (0.898)(0.938, 0.859)] [G loss: 4.760] [G acc: 0.000]\n",
      "348 [D loss: (0.245)(R 0.333, F 0.157)] [D acc: (0.914)(0.859, 0.969)] [G loss: 4.585] [G acc: 0.000]\n",
      "349 [D loss: (0.122)(R 0.120, F 0.124)] [D acc: (0.961)(0.938, 0.984)] [G loss: 3.711] [G acc: 0.031]\n",
      "350 [D loss: (0.238)(R 0.115, F 0.362)] [D acc: (0.906)(0.969, 0.844)] [G loss: 6.031] [G acc: 0.000]\n",
      "351 [D loss: (0.553)(R 0.864, F 0.242)] [D acc: (0.773)(0.625, 0.922)] [G loss: 4.886] [G acc: 0.000]\n",
      "352 [D loss: (0.337)(R 0.238, F 0.436)] [D acc: (0.859)(0.906, 0.812)] [G loss: 4.092] [G acc: 0.000]\n",
      "353 [D loss: (0.259)(R 0.170, F 0.348)] [D acc: (0.883)(0.922, 0.844)] [G loss: 4.463] [G acc: 0.000]\n",
      "354 [D loss: (0.484)(R 0.454, F 0.514)] [D acc: (0.773)(0.797, 0.750)] [G loss: 4.201] [G acc: 0.016]\n",
      "355 [D loss: (0.319)(R 0.437, F 0.202)] [D acc: (0.859)(0.812, 0.906)] [G loss: 3.817] [G acc: 0.000]\n",
      "356 [D loss: (0.437)(R 0.259, F 0.615)] [D acc: (0.812)(0.891, 0.734)] [G loss: 4.026] [G acc: 0.000]\n",
      "357 [D loss: (0.376)(R 0.413, F 0.339)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.763] [G acc: 0.016]\n",
      "358 [D loss: (0.267)(R 0.270, F 0.264)] [D acc: (0.898)(0.875, 0.922)] [G loss: 3.570] [G acc: 0.000]\n",
      "359 [D loss: (0.221)(R 0.218, F 0.224)] [D acc: (0.914)(0.922, 0.906)] [G loss: 4.256] [G acc: 0.000]\n",
      "360 [D loss: (0.454)(R 0.480, F 0.427)] [D acc: (0.797)(0.797, 0.797)] [G loss: 5.304] [G acc: 0.000]\n",
      "361 [D loss: (0.251)(R 0.357, F 0.146)] [D acc: (0.914)(0.859, 0.969)] [G loss: 4.530] [G acc: 0.000]\n",
      "362 [D loss: (0.224)(R 0.171, F 0.276)] [D acc: (0.922)(0.938, 0.906)] [G loss: 5.174] [G acc: 0.000]\n",
      "363 [D loss: (0.300)(R 0.301, F 0.299)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.646] [G acc: 0.016]\n",
      "364 [D loss: (0.243)(R 0.243, F 0.242)] [D acc: (0.898)(0.906, 0.891)] [G loss: 4.826] [G acc: 0.016]\n",
      "365 [D loss: (0.428)(R 0.468, F 0.388)] [D acc: (0.828)(0.812, 0.844)] [G loss: 3.581] [G acc: 0.016]\n",
      "366 [D loss: (0.293)(R 0.246, F 0.341)] [D acc: (0.859)(0.891, 0.828)] [G loss: 4.576] [G acc: 0.000]\n",
      "367 [D loss: (0.456)(R 0.434, F 0.478)] [D acc: (0.805)(0.781, 0.828)] [G loss: 4.265] [G acc: 0.000]\n",
      "368 [D loss: (0.398)(R 0.419, F 0.377)] [D acc: (0.844)(0.828, 0.859)] [G loss: 4.927] [G acc: 0.000]\n",
      "369 [D loss: (0.387)(R 0.543, F 0.231)] [D acc: (0.859)(0.781, 0.938)] [G loss: 3.882] [G acc: 0.000]\n",
      "370 [D loss: (0.221)(R 0.161, F 0.280)] [D acc: (0.914)(0.953, 0.875)] [G loss: 4.247] [G acc: 0.000]\n",
      "371 [D loss: (0.408)(R 0.278, F 0.538)] [D acc: (0.844)(0.891, 0.797)] [G loss: 4.263] [G acc: 0.000]\n",
      "372 [D loss: (0.306)(R 0.437, F 0.175)] [D acc: (0.859)(0.781, 0.938)] [G loss: 4.059] [G acc: 0.000]\n",
      "373 [D loss: (0.312)(R 0.207, F 0.418)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.724] [G acc: 0.000]\n",
      "374 [D loss: (0.241)(R 0.341, F 0.142)] [D acc: (0.914)(0.859, 0.969)] [G loss: 4.078] [G acc: 0.000]\n",
      "375 [D loss: (0.193)(R 0.152, F 0.235)] [D acc: (0.945)(0.953, 0.938)] [G loss: 5.055] [G acc: 0.000]\n",
      "376 [D loss: (0.298)(R 0.385, F 0.212)] [D acc: (0.875)(0.812, 0.938)] [G loss: 3.980] [G acc: 0.000]\n",
      "377 [D loss: (0.254)(R 0.142, F 0.366)] [D acc: (0.898)(0.938, 0.859)] [G loss: 4.962] [G acc: 0.000]\n",
      "378 [D loss: (0.405)(R 0.567, F 0.242)] [D acc: (0.820)(0.750, 0.891)] [G loss: 4.655] [G acc: 0.000]\n",
      "379 [D loss: (0.497)(R 0.344, F 0.651)] [D acc: (0.773)(0.844, 0.703)] [G loss: 6.038] [G acc: 0.000]\n",
      "380 [D loss: (0.740)(R 1.318, F 0.162)] [D acc: (0.648)(0.344, 0.953)] [G loss: 3.993] [G acc: 0.000]\n",
      "381 [D loss: (0.330)(R 0.296, F 0.364)] [D acc: (0.859)(0.891, 0.828)] [G loss: 3.205] [G acc: 0.016]\n",
      "382 [D loss: (0.341)(R 0.237, F 0.445)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.166] [G acc: 0.016]\n",
      "383 [D loss: (0.260)(R 0.198, F 0.323)] [D acc: (0.883)(0.922, 0.844)] [G loss: 3.389] [G acc: 0.000]\n",
      "384 [D loss: (0.530)(R 0.494, F 0.565)] [D acc: (0.719)(0.734, 0.703)] [G loss: 3.389] [G acc: 0.031]\n",
      "385 [D loss: (0.420)(R 0.460, F 0.381)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.925] [G acc: 0.016]\n",
      "386 [D loss: (0.374)(R 0.499, F 0.250)] [D acc: (0.844)(0.750, 0.938)] [G loss: 2.786] [G acc: 0.062]\n",
      "387 [D loss: (0.474)(R 0.355, F 0.594)] [D acc: (0.750)(0.828, 0.672)] [G loss: 3.214] [G acc: 0.000]\n",
      "388 [D loss: (0.414)(R 0.380, F 0.448)] [D acc: (0.789)(0.828, 0.750)] [G loss: 3.292] [G acc: 0.000]\n",
      "389 [D loss: (0.477)(R 0.594, F 0.359)] [D acc: (0.805)(0.688, 0.922)] [G loss: 2.850] [G acc: 0.016]\n",
      "390 [D loss: (0.213)(R 0.233, F 0.192)] [D acc: (0.914)(0.875, 0.953)] [G loss: 3.126] [G acc: 0.016]\n",
      "391 [D loss: (0.296)(R 0.252, F 0.340)] [D acc: (0.891)(0.922, 0.859)] [G loss: 4.267] [G acc: 0.000]\n",
      "392 [D loss: (0.399)(R 0.530, F 0.267)] [D acc: (0.836)(0.750, 0.922)] [G loss: 3.626] [G acc: 0.000]\n",
      "393 [D loss: (0.417)(R 0.265, F 0.568)] [D acc: (0.805)(0.875, 0.734)] [G loss: 3.800] [G acc: 0.000]\n",
      "394 [D loss: (0.348)(R 0.486, F 0.210)] [D acc: (0.891)(0.812, 0.969)] [G loss: 3.565] [G acc: 0.016]\n",
      "395 [D loss: (0.412)(R 0.330, F 0.495)] [D acc: (0.789)(0.844, 0.734)] [G loss: 4.196] [G acc: 0.000]\n",
      "396 [D loss: (0.317)(R 0.459, F 0.176)] [D acc: (0.891)(0.828, 0.953)] [G loss: 3.153] [G acc: 0.016]\n",
      "397 [D loss: (0.234)(R 0.230, F 0.237)] [D acc: (0.914)(0.922, 0.906)] [G loss: 3.353] [G acc: 0.000]\n",
      "398 [D loss: (0.313)(R 0.263, F 0.364)] [D acc: (0.844)(0.859, 0.828)] [G loss: 3.702] [G acc: 0.016]\n",
      "399 [D loss: (0.476)(R 0.409, F 0.543)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.415] [G acc: 0.000]\n",
      "400 [D loss: (0.327)(R 0.346, F 0.309)] [D acc: (0.859)(0.812, 0.906)] [G loss: 3.347] [G acc: 0.016]\n",
      "401 [D loss: (0.356)(R 0.412, F 0.299)] [D acc: (0.836)(0.797, 0.875)] [G loss: 4.931] [G acc: 0.000]\n",
      "402 [D loss: (0.222)(R 0.284, F 0.161)] [D acc: (0.898)(0.844, 0.953)] [G loss: 3.806] [G acc: 0.000]\n",
      "403 [D loss: (0.078)(R 0.072, F 0.084)] [D acc: (0.992)(0.984, 1.000)] [G loss: 4.078] [G acc: 0.000]\n",
      "404 [D loss: (0.188)(R 0.190, F 0.186)] [D acc: (0.922)(0.906, 0.938)] [G loss: 4.138] [G acc: 0.016]\n",
      "405 [D loss: (0.419)(R 0.306, F 0.532)] [D acc: (0.820)(0.875, 0.766)] [G loss: 4.711] [G acc: 0.000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dad7351e4e33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m gan.train(x_train, batch_size = BATCH_SIZE, epochs = EPOCHS, run_folder = RUN_FOLDER, \n\u001b[1;32m----> 2\u001b[1;33m           \u001b[0mprint_every_n_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Data\\PROG\\Github\\tensorflow\\tensorflow\\Basic GAN\\GANstructure.ipynb\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batch, using_generator)\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[1;31m#self.sample_images(run_folder)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'weights/weights-%d.h5'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'weights/weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m                 \u001b[1;31m#self.save_model(run_folder)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Data\\appData2\\anaconda3\\envs\\gans\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_wrapper\u001b[1;34m(obj, filepath, overwrite, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0msave_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msave_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Data\\appData2\\anaconda3\\envs\\gans\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Data\\appData2\\anaconda3\\envs\\gans\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[1;34m(group, layers)\u001b[0m\n\u001b[0;32m    764\u001b[0m                 \u001b[0mparam_dset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m                 \u001b[0mparam_dset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Data\\appData2\\anaconda3\\envs\\gans\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, args, val)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNLIMITED\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_direct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.train(x_train, batch_size = BATCH_SIZE, epochs = EPOCHS, run_folder = RUN_FOLDER, \n",
    "          print_every_n_batch = PRINT_EVERY_N_BATCHES\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganskernel",
   "language": "python",
   "name": "ganskernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
